{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.LSA_Dataset import LSA_Dataset\n",
    "from data.transforms import get_frames_reduction_transform, get_roi_selector_transform, get_keypoint_format_transform, get_text_to_tensor_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/datasets/cn_sordos_db/data/cuts'\n",
    "max_frames = 75\n",
    "keypoints_to_use = [i for i in range(94, 136)]\n",
    "\n",
    "dataset = LSA_Dataset(\n",
    "    root,\n",
    "    load_videos = False,\n",
    "    frame_transform = get_roi_selector_transform(128,128),\n",
    "    video_transform = get_frames_reduction_transform(max_frames),\n",
    "    keypoints_transform = get_frames_reduction_transform(max_frames),\n",
    "    keypoints_transform_each = get_keypoint_format_transform(keypoints_to_use)\n",
    "    )\n",
    "dataset.label_transform = get_text_to_tensor_transform(dataset.vocab.__getitem__(\"<bos>\"), dataset.vocab.__getitem__(\"<eos>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(0, dataset.__len__())\n",
    "sample = dataset.__getitem__(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/pdalbianco/cn_sordos_model/test.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.62/home/pdalbianco/cn_sordos_model/test.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(sample[\u001b[39m3\u001b[39;49m]\u001b[39m.\u001b[39;49mtokens)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.62/home/pdalbianco/cn_sordos_model/test.ipynb#ch0000003vscode-remote?line=1'>2</a>\u001b[0m clip \u001b[39m=\u001b[39m sample[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.62/home/pdalbianco/cn_sordos_model/test.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[F, C, H, W]\u001b[39m\u001b[39m\"\u001b[39m, clip\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "print(sample[3])\n",
    "clip = sample[1]\n",
    "print(\"[F, C, H, W]\", clip.size())\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(20):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    iclip = clip[i, ...].permute(1,2,0)\n",
    "    plt.imshow(iclip)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   2, 1469, 9092,    6,   16,  967,   28,    6,   27,    8, 5158,    5,\n",
      "           3])\n",
      "75\n",
      "torch.Size([3, 42])\n",
      "torch.Size([75, 3, 42])\n"
     ]
    }
   ],
   "source": [
    "from torch import stack\n",
    "\n",
    "print(sample[2])\n",
    "\n",
    "print(len(keys))\n",
    "print(keys[0].size())\n",
    "\n",
    "vid_keys = stack(keys)\n",
    "print(vid_keys.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from train import get_collate_fn\n",
    "\n",
    "dl = DataLoader(dataset, 12, collate_fn=get_collate_fn(dataset.vocab.__getitem__(\"<pad>\")))\n",
    "for b in dl:\n",
    "    print(type(b[0][0]))\n",
    "    print(len(b[0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from model.modules.KeypointsEmbedding import KeypointsEmbedding\n",
    "from model.modules.PositionalEncoding import PositionalEncoding\n",
    "from model.modules.TokenEmbedding import TokenEmbedding\n",
    "\n",
    "\n",
    "class KeypointModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                src_max_len: int,\n",
    "                tgt_max_len: int,\n",
    "                keys_len: int,\n",
    "                tgt_vocab_size: int,\n",
    "                kernel_size: int = 5,\n",
    "                emb_size: int = 64,\n",
    "                keys_initial_emb_size: int = 128,\n",
    "                ):\n",
    "        super(KeypointModel, self).__init__()\n",
    "\n",
    "        # in_features is the result of flattening the input of (x,y,c).(k1, ..., k42)\n",
    "        self.keys_emb = KeypointsEmbedding(keys_len=keys_len, kernel_size=kernel_size, emb_size=emb_size, keys_initial_emb_size=keys_initial_emb_size)\n",
    "        self.src_pe = PositionalEncoding(emb_size=emb_size, max_len=(src_max_len - kernel_size + 1))\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.tgt_pe = PositionalEncoding(emb_size=emb_size, max_len=tgt_max_len)\n",
    "        self.transformer = nn.Transformer(d_model=emb_size)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                src: List[Tensor],\n",
    "                tgt: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.src_pe(self.keys_emb(src))\n",
    "        tgt_emb = self.tgt_pe(self.tgt_tok_emb(tgt))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.src_pe(self.keys_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.tgt_pe(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_MAX_LEN = max_frames\n",
    "TGT_VOCAB_SIZE = len(dataset.vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "\n",
    "transformer = KeypointModel(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(model(vid_keys, None).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "print(generate_square_subsequent_mask(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pdalbianco/cn_sordos_model/test.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.62/home/pdalbianco/cn_sordos_model/test.ipynb#ch0000005vscode-remote?line=0'>1</a>\u001b[0m loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.62/home/pdalbianco/cn_sordos_model/test.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m data: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mkeypoints\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.62/home/pdalbianco/cn_sordos_model/test.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m loader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=12)\n",
    "data: Dict[str, Any] = {'path': [], 'clip': [], 'keypoints': [], 'label': []}\n",
    "for batch in loader:\n",
    "    for i in range(len(batch['path'])):\n",
    "        data['path'].append(batch['path'][i])\n",
    "        data['clip'].append(batch['clip'][i].size())\n",
    "        #data['keypoints'].append(batch['end'][i])\n",
    "        data['label'].append(batch['label'][i])\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c39aee66d4694ab6179e36ecdc8652752b93bf45b611fd43c1db6ceb5e4ba9d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
